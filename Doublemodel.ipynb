{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d05ff88",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#import statements, weight definitions, data classes, rounding function\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data class\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#Data loaders (to look at data) and deal with data (training and testing)\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def binary_acc(y_pred, y_test): \n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=random.rand(1000, 5)\n",
    "\n",
    "np.random.normal(0, 1, size=(1000, 10))\n",
    "\n",
    "\n",
    "y=[]\n",
    "for i in range(1000):\n",
    "    y.append(random.binomial(1, 0.5, size=None))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f554690d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#model class 5 layers, then an output layer\n",
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(5, 5) \n",
    "        self.layer_2 = nn.Linear(5, 5) \n",
    "        self.layer_3 = nn.Linear(5, 5)\n",
    "        self.layer_4 = nn.Linear(5, 5)\n",
    "        self.layer_5 = nn.Linear(5, 5) \n",
    "        self.layer_out = nn.Linear(5, 1) #output layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(5)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(self.layer_4(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.relu(self.layer_5(x))\n",
    "        x = self.batchnorm5(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e196afae",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#parallel model (two 5 layer models, with output going into new output layer)\n",
    "class parallelmodel(nn.Module):\n",
    "    def __init__(self, originalmodel):\n",
    "        super(parallelmodel, self).__init__()\n",
    "        self.layer1_1 = copy.deepcopy(originalmodel.layer_1)\n",
    "        self.layer1_2 = copy.deepcopy(originalmodel.layer_2)\n",
    "        self.layer1_3 = copy.deepcopy(originalmodel.layer_3)\n",
    "        self.layer1_4 = copy.deepcopy(originalmodel.layer_4)\n",
    "        self.layer1_5 = copy.deepcopy(originalmodel.layer_5)\n",
    "        self.layer1_out = copy.deepcopy(originalmodel.layer_out)\n",
    "        \n",
    "        self.relu1 = copy.deepcopy(originalmodel.relu)        \n",
    "        self.dropout1 = copy.deepcopy(originalmodel.dropout)\n",
    "        \n",
    "        self.batchnorm1_1 = copy.deepcopy(originalmodel.batchnorm1)\n",
    "        self.batchnorm1_2 = copy.deepcopy(originalmodel.batchnorm2)\n",
    "        self.batchnorm1_3 = copy.deepcopy(originalmodel.batchnorm3)\n",
    "        self.batchnorm1_4 = copy.deepcopy(originalmodel.batchnorm4)\n",
    "        self.batchnorm1_5 = copy.deepcopy(originalmodel.batchnorm5)\n",
    "        #self.batchnorm1_out=nn.BatchNorm1d(1)\n",
    "\n",
    "        \n",
    "        self.batchnorm2_1 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2_2 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2_3 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2_4 = nn.BatchNorm1d(5)\n",
    "        self.batchnorm2_5 = nn.BatchNorm1d(5)\n",
    "        #self.batchnorm2_out=nn.BatchNorm1d()\n",
    "        \n",
    "        self.layer2_1 = nn.Linear(5, 5) \n",
    "        self.layer2_2 = nn.Linear(5, 5) \n",
    "        self.layer2_3 = nn.Linear(5, 5)\n",
    "        self.layer2_4 = nn.Linear(5, 5)\n",
    "        self.layer2_5 = nn.Linear(5, 5) \n",
    "        self.layer2_out = nn.Linear(5, 1) \n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.outputlayer= nn.Linear(2, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu1(self.layer1_1(inputs))\n",
    "        x = self.batchnorm1_1(x)\n",
    "        x = self.relu1(self.layer1_2(x))\n",
    "        x = self.batchnorm1_2(x)\n",
    "        x = self.relu1(self.layer1_3(x))\n",
    "        x = self.batchnorm1_3(x)\n",
    "        x = self.relu1(self.layer1_4(x))\n",
    "        x = self.batchnorm1_4(x)\n",
    "        x = self.relu1(self.layer1_5(x))\n",
    "        x = self.batchnorm1_5(x) \n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer1_out(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        \n",
    "        \n",
    "        y = self.relu2(self.layer2_1(inputs))\n",
    "        y = self.batchnorm2_1(y)\n",
    "        y = self.relu2(self.layer2_2(y))\n",
    "        y = self.batchnorm2_2(y)\n",
    "        y = self.relu2(self.layer2_3(y))\n",
    "        y = self.batchnorm2_3(y)\n",
    "        y = self.relu2(self.layer2_4(y))\n",
    "        y = self.batchnorm2_4(y)\n",
    "        y = self.relu2(self.layer2_5(y))\n",
    "        y = self.batchnorm2_5(y)        \n",
    "        y = self.dropout2(y)\n",
    "        y = self.layer2_out(y)\n",
    "        y=torch.sigmoid(y)\n",
    "        \n",
    "        z=self.outputlayer(torch.cat([x, y], dim=1))\n",
    "        return z\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd55f044",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#groundtruth model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = StandardScaler()\n",
    "X_allscaled = scaler.fit_transform(X) #scales data\n",
    "X_formatted = testData(torch.FloatTensor(X_allscaled))\n",
    "EPOCHS = 50 #number of passes of whole data\n",
    "BATCH_SIZE = 64 #size of data going through at once\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Truthtrain=trainData(torch.FloatTensor(X_allscaled), \n",
    "                       torch.FloatTensor(y))\n",
    "\n",
    "\n",
    "Truthloadertrain= DataLoader(dataset=Truthtrain, batch_size=BATCH_SIZE, shuffle=True)\n",
    "GroundTruth = binaryClassification()\n",
    "\n",
    "\n",
    "GroundTruth.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "truthoptimizer = optim.Adam(GroundTruth.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#print(list(model.parameters())[0])\n",
    "\n",
    "\n",
    "\n",
    "baddata=True\n",
    "while(baddata):\n",
    "\n",
    "    \n",
    "\n",
    "    GroundTruth.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch in Truthloadertrain:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            truthoptimizer.zero_grad()\n",
    "\n",
    "            y_pred = GroundTruth(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            truthoptimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Truthtest = testData(torch.FloatTensor(X_allscaled))\n",
    "    Truthloader = DataLoader(dataset=Truthtest, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    truth_list = []\n",
    "    GroundTruth.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in Truthloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_truth = GroundTruth(X_batch)\n",
    "            y_truth = torch.sigmoid(y_truth)\n",
    "            y_truthtag = torch.round(y_truth)\n",
    "            truth_list.append(y_truthtag.cpu().numpy())\n",
    "\n",
    "    y = [a.squeeze().tolist() for a in truth_list] #new truth values\n",
    "    #split data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "    #print(y_train)\n",
    "    #rescale data \n",
    "    scaler = StandardScaler()\n",
    "    truthcounter=0\n",
    "    for i in y:\n",
    "        if i==1:\n",
    "            truthcounter=truthcounter+1\n",
    "    \n",
    "    \n",
    "    #print(truthcounter)\n",
    "    if truthcounter/len(y)<.8 and truthcounter/len(y)>.2:\n",
    "        baddata=False\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data= trainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "\n",
    "\n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "\n",
    "#data loader initiation\n",
    "train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b14074",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# correct:  234  out of  330\n",
      "Full model statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.71      0.70       157\n",
      "         1.0       0.73      0.71      0.72       173\n",
      "\n",
      "    accuracy                           0.71       330\n",
      "   macro avg       0.71      0.71      0.71       330\n",
      "weighted avg       0.71      0.71      0.71       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train first model\n",
    "trained = binaryClassification()\n",
    "trained.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(trained.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#w1=(trained.layer_1.weight)\n",
    "#w2=(trained.layer_2.weight)\n",
    "#w3=(trained.layer_3.weight)\n",
    "#w4=(trained.layer_4.weight)\n",
    "#w5=(trained.layer_5.weight)\n",
    "#print(w1)\n",
    "#print(w2)\n",
    "#print(w3)\n",
    "#print(w4)\n",
    "#print(w5)\n",
    "#print(trained.layer_out.weight)\n",
    "#train trained model\n",
    "trained.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = trained(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "y_fullmodelpred_list =[]\n",
    "trained.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = trained(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_fullmodelpred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_fullmodelpred_list = [a.squeeze().tolist() for a in y_fullmodelpred_list]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "fullmodelcounter=0\n",
    "for i in range(len(y_fullmodelpred_list)):\n",
    "    if y_fullmodelpred_list[i]==y_test[i]:\n",
    "        fullmodelcounter=fullmodelcounter+1       \n",
    "\n",
    "print(\"# correct: \",fullmodelcounter, \" out of \", len(y_test))\n",
    "        \n",
    "print(\"Full model statistics\")\n",
    "print(classification_report(y_test, y_fullmodelpred_list))    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a742d7a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# correct:  257  out of  330\n",
      "Full model statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.73       157\n",
      "         1.0       0.73      0.92      0.81       173\n",
      "\n",
      "    accuracy                           0.78       330\n",
      "   macro avg       0.80      0.77      0.77       330\n",
      "weighted avg       0.80      0.78      0.77       330\n",
      "\n",
      "output weight:  Parameter containing:\n",
      "tensor([[1.1769, 0.6490]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#train parralel model\n",
    "parralel = parallelmodel(trained)\n",
    "parralel.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "parralel.outputlayer.weight=torch.nn.Parameter(data=torch.tensor([[.7,0.3]]), requires_grad=True)\n",
    "\n",
    "\n",
    "paroptimizer = optim.Adam(parralel.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "#EPOCHS = 300 #number of passes of whole data\n",
    "#BATCH_SIZE = 16 #size of data going through at once\n",
    "\n",
    "\n",
    "\n",
    "#train trained model\n",
    "parralel.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        paroptimizer.zero_grad()\n",
    "        \n",
    "        y_pred = parralel(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        paroptimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    #print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "y_parmodelpred_list =[]\n",
    "parralel.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = parralel(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_parmodelpred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_parmodelpred_list = [a.squeeze().tolist() for a in y_parmodelpred_list]\n",
    "\n",
    "\n",
    "#print(y_parmodelpred_list)\n",
    "    \n",
    "parmodelcounter=0\n",
    "for i in range(len(y_fullmodelpred_list)):\n",
    "    if y_parmodelpred_list[i]==y_test[i]:\n",
    "        parmodelcounter=parmodelcounter+1       \n",
    "\n",
    "print(\"# correct: \",parmodelcounter, \" out of \", len(y_test))\n",
    "        \n",
    "print(\"Full model statistics\")\n",
    "print(classification_report(y_test, y_parmodelpred_list))    \n",
    "    \n",
    "    \n",
    "print(\"output weight: \", parralel.outputlayer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2df777",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#print(w1)\n",
    "#print(w2)\n",
    "#print(w3)\n",
    "#print(w4)\n",
    "#print(w5)\n",
    "#print(trained.layer_out.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a880e22f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained stats:\n",
      "Tained half model;\n",
      "Number correct full model:  157  out of  330\n",
      "precision=(# of correct)/(# of guessed)\n",
      "recall=(# of correct)/(total)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.64       157\n",
      "         1.0       0.00      0.00      0.00       173\n",
      "\n",
      "    accuracy                           0.48       330\n",
      "   macro avg       0.24      0.50      0.32       330\n",
      "weighted avg       0.23      0.48      0.31       330\n",
      "\n",
      "untrained half stats:\n",
      "untained half model;\n",
      "Number correct full model:  157  out of  330\n",
      "precision=(# of correct)/(# of guessed)\n",
      "recall=(# of correct)/(total)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.64       157\n",
      "         1.0       0.00      0.00      0.00       173\n",
      "\n",
      "    accuracy                           0.48       330\n",
      "   macro avg       0.24      0.50      0.32       330\n",
      "weighted avg       0.23      0.48      0.31       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#half model stats\n",
    "\n",
    "halfmodeltrained=binaryClassification()\n",
    "halfmodellow=binaryClassification()\n",
    "\n",
    "halfmodeltrained.layer_1 = copy.deepcopy(parralel.layer1_1)\n",
    "halfmodeltrained.layer_2 = copy.deepcopy(parralel.layer1_2)\n",
    "halfmodeltrained.layer_3 = copy.deepcopy(parralel.layer1_3)\n",
    "halfmodeltrained.layer_4 = copy.deepcopy(parralel.layer1_4)\n",
    "halfmodeltrained.layer_5 = copy.deepcopy(parralel.layer1_5)\n",
    "halfmodeltrained.layer_out = copy.deepcopy(parralel.layer1_out)\n",
    "\n",
    "\n",
    "halfmodellow.layer_1 = copy.deepcopy(parralel.layer2_1)\n",
    "halfmodellow.layer_2 = copy.deepcopy(parralel.layer2_2)\n",
    "halfmodellow.layer_3 = copy.deepcopy(parralel.layer2_3)\n",
    "halfmodellow.layer_4 = copy.deepcopy(parralel.layer2_4)\n",
    "halfmodellow.layer_5 = copy.deepcopy(parralel.layer2_5)\n",
    "halfmodellow.layer_out = copy.deepcopy(parralel.layer2_out)\n",
    "\n",
    "\n",
    "print(\"pretrained stats:\")\n",
    "\n",
    "\n",
    "\n",
    "y_pretrianedlist =[]\n",
    "halfmodeltrained.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = halfmodeltrained(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pretrianedlist.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_pretrianedlist = [a.squeeze().tolist() for a in y_pretrianedlist]\n",
    "\n",
    "\n",
    "\n",
    "correctcounter=0\n",
    "for i in range(len(y_pretrianedlist)):\n",
    "    if y_pretrianedlist[i]==y_test[i]:\n",
    "        correctcounter=correctcounter+1 \n",
    "        \n",
    "        \n",
    "print(\"Tained half model;\")\n",
    "print(\"Number correct full model: \", correctcounter, \" out of \", len(y_test))\n",
    "print(\"precision=(# of correct)/(# of guessed)\")\n",
    "print(\"recall=(# of correct)/(total)\")\n",
    "print(classification_report(y_test, y_pretrianedlist))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"untrained half stats:\")\n",
    "y_untrianedlist =[]\n",
    "halfmodellow.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = halfmodellow(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_untrianedlist.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_untrianedlist = [a.squeeze().tolist() for a in y_untrianedlist]\n",
    "\n",
    "\n",
    "\n",
    "correctcounter=0\n",
    "for i in range(len(y_untrianedlist)):\n",
    "    if y_untrianedlist[i]==y_test[i]:\n",
    "        correctcounter=correctcounter+1 \n",
    "        \n",
    "        \n",
    "print(\"untained half model;\")\n",
    "print(\"Number correct full model: \", correctcounter, \" out of \", len(y_test))\n",
    "print(\"precision=(# of correct)/(# of guessed)\")\n",
    "print(\"recall=(# of correct)/(total)\")\n",
    "print(classification_report(y_test, y_untrianedlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f137a5bc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of training routines:2\n",
      "training routine #:  1\n",
      "# correct:  168  out of  330\n",
      "Full model statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.31      0.38       157\n",
      "         1.0       0.52      0.69      0.59       173\n",
      "\n",
      "    accuracy                           0.51       330\n",
      "   macro avg       0.50      0.50      0.49       330\n",
      "weighted avg       0.50      0.51      0.49       330\n",
      "\n",
      "output weight:  Parameter containing:\n",
      "tensor([[0.9721, 0.6614]], requires_grad=True)\n",
      "training routine #:  2\n",
      "# correct:  155  out of  330\n",
      "Full model statistics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.27      0.33       157\n",
      "         1.0       0.50      0.65      0.56       173\n",
      "\n",
      "    accuracy                           0.47       330\n",
      "   macro avg       0.45      0.46      0.45       330\n",
      "weighted avg       0.46      0.47      0.45       330\n",
      "\n",
      "output weight:  Parameter containing:\n",
      "tensor([[0.9296, 0.6703]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#multiple routines\n",
    "val = input(\"Enter number of training routines:\")\n",
    "for k in range(int(val)):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X=random.rand(1000, 5)\n",
    "\n",
    "    np.random.normal(0, 1, size=(1000, 10))\n",
    "\n",
    "\n",
    "    y=[]\n",
    "    for i in range(1000):\n",
    "        y.append(random.binomial(1, 0.5, size=None))\n",
    "\n",
    "    \n",
    "    truth_list = []\n",
    "    GroundTruth.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in Truthloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_truth = GroundTruth(X_batch)\n",
    "            y_truth = torch.sigmoid(y_truth)\n",
    "            y_truthtag = torch.round(y_truth)\n",
    "            truth_list.append(y_truthtag.cpu().numpy())\n",
    "\n",
    "    y = [a.squeeze().tolist() for a in truth_list] #new truth values\n",
    "    #split data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_data= trainData(torch.FloatTensor(X_train), \n",
    "                           torch.FloatTensor(y_train))\n",
    "\n",
    "\n",
    "\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "\n",
    "    #data loader initiation\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #train trained model\n",
    "    parralel.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            paroptimizer.zero_grad()\n",
    "\n",
    "            y_pred = parralel(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            paroptimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "        #print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_parmodelpred_list =[]\n",
    "    parralel.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = parralel(X_batch)\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_parmodelpred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_parmodelpred_list = [a.squeeze().tolist() for a in y_parmodelpred_list]\n",
    "\n",
    "\n",
    "    #print(y_parmodelpred_list)\n",
    "\n",
    "    parmodelcounter=0\n",
    "    for i in range(len(y_fullmodelpred_list)):\n",
    "        if y_parmodelpred_list[i]==y_test[i]:\n",
    "            parmodelcounter=parmodelcounter+1       \n",
    "    print(\"training routine #: \", k+1)\n",
    "    print(\"# correct: \",parmodelcounter, \" out of \", len(y_test))\n",
    "\n",
    "    print(\"Full model statistics\")\n",
    "    print(classification_report(y_test, y_parmodelpred_list))    \n",
    "    print(\"output weight: \", parralel.outputlayer.weight)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54494ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
