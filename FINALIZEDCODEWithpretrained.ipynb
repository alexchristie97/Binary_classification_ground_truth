{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05ff88",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss for pretraining:  0.06975987471996632\n",
      "epoch loss for pretraining:  0.006088731123782054\n",
      "0.34576052\n",
      "0.2398264\n",
      "0.1670857\n",
      "0.14624049\n",
      "0.12048188\n",
      "0.1347118\n",
      "0.12698027\n",
      "0.15311857\n",
      "0.10369847\n",
      "0.10565291\n",
      "0.13205941\n",
      "0.119047746\n",
      "0.110962115\n"
     ]
    }
   ],
   "source": [
    "#import statements, hyper params, models\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from numpy import random\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import math\n",
    "import time\n",
    "\n",
    "THRESH_HOLD=.002          #MSE value when we consider the second model to have \"cracked\" the first\n",
    "EPOCHS = 50                #number of passes of whole data\n",
    "BATCH_SIZE =  50            #size of data going through at once\n",
    "LEARNING_RATE = 0.001        #how much we shift parameters during back prop\n",
    "NEURONS=1024                   #number of nodes in each layer\n",
    "INPUTS=1                 #number of input vars\n",
    "OUTPUT=1                    #number of output\n",
    "XSIZE=  100                 #size of each data set generated\n",
    "BAD_LOSS_PASSES=1000000        #number of times we have small loss until we stop code\n",
    "NO_TIME_INCREASES_STOP=100   #number of interations with no time increase until we stop code\n",
    "TESTMODULE_LAYERS=5        #number of layers in cracking module\n",
    "GROUND_TRUTH_LAYERS=5      #number of layers in ground thruth models\n",
    "LINCOMBO=False               #wether to take best new time or linear combination\n",
    "activation=nn.ReLU()    #activation  \n",
    "TRUTHMODELSGENERATED=1     #number of perturbed models\n",
    "TESTMODELSGENERATED=1\n",
    "pretrained=True\n",
    "DATATYPE=0                #0=data in [-1 ,1], rescaled; 1=sparse data; 2=periodic\n",
    "def get_Xvals():\n",
    "    if DATATYPE==0:\n",
    "        return np.random.uniform( size=(XSIZE, INPUTS))*2*np.pi\n",
    "    elif DATATYPE==1:\n",
    "        sparsedata=(np.random.uniform( size=(XSIZE, INPUTS)))*2*np.pi\n",
    "        for i in sparsedata:\n",
    "            for j in i:\n",
    "                if np.random.randint(0,2)==0:\n",
    "                    j=0\n",
    "        for i in sparsedata:\n",
    "            for j in i:\n",
    "                if np.random.randint(0,2)==0:\n",
    "                    j=0\n",
    "        return sparsedata\n",
    "    else:\n",
    "        return np.random.normal(size=(XSIZE, INPUTS))\n",
    "        \n",
    "\n",
    "                \n",
    "    \n",
    "def getdata_XY():\n",
    "    X=get_Xvals()\n",
    "    if DATATYPE==2:\n",
    "        dummy=[(i%1)*2*np.pi for i in X]\n",
    "    else:\n",
    "        dummy=X\n",
    "    test_data = testData(torch.FloatTensor(dummy))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    y_truthlist =[]\n",
    "    \n",
    "    \n",
    "    GroundTruth.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = GroundTruth(X_batch)\n",
    "            y_truthlist.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_truthlist = [a.squeeze().tolist() for a in y_truthlist]\n",
    "    return X, y_truthlist\n",
    "\n",
    "def get_yvals(model, X_data):\n",
    "    if DATATYPE==2:\n",
    "        X_data=[(i%1)*2*np.pi for i in X]\n",
    "    model.eval()\n",
    "    test_data = testData(torch.FloatTensor(X_data))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    y_truthlist=[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_truthlist.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_truthlist = [a.squeeze().tolist() for a in y_truthlist]\n",
    "    #print(\"MAX-MIN of yvals:\", max(y_truthlist)-min(y_truthlist))\n",
    "    #print(max(y_truthlist))\n",
    "    #print(min(y_truthlist))\n",
    "    return y_truthlist\n",
    "\n",
    "#Data class\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "#Data loaders (to look at data) and deal with data (training and testing)\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "#trains a network, returns time spent training \n",
    "def trainnetwork(network, train_loader, optimizer, Time=True):\n",
    "    network.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    network.train()\n",
    "    current_epoch=1\n",
    "    last_epoch=0\n",
    "    epoch_diff_counter=0\n",
    "    total_counter=0\n",
    "    while (epoch_diff_counter<EPOCHS and total_counter<EPOCHS*10) and current_epoch>THRESH_HOLD:\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = network(X_batch)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()        \n",
    "        \n",
    "        last_epoch=current_epoch\n",
    "        current_epoch=epoch_loss/len(train_loader)\n",
    "        if abs(last_epoch-current_epoch)<THRESH_HOLD/10:\n",
    "            epoch_diff_counter=epoch_diff_counter+1\n",
    "        else:\n",
    "            epoch_diff_counter=0\n",
    "        total_counter=total_counter+1\n",
    "            \n",
    "        #print(\"epoch loss: \", epoch_loss/len(train_loader))\n",
    "            \n",
    "\n",
    "#returns next ground truth model\n",
    "#model obtained by taking average of models with better times than previous model \n",
    "def get_new_GroundTruth_lincombo(model_list, time_list, time):\n",
    "    better_model_list=[]\n",
    "    for (i, model) in zip(time_list, model_list):\n",
    "        if i>time:\n",
    "            better_model_list.append(model)\n",
    "    \n",
    "    newGroundTruth=GroundTruth.get_copy()\n",
    "    \n",
    "    layer_weight=torch.tensor([[0]*INPUTS]*NEURONS, requires_grad=False)\n",
    "    for i in better_model_list:\n",
    "        layer_weight=layer_weight+i.layer_input.weight/(len(better_model_list))\n",
    "        \n",
    "    layer_weight.requires_grad_()    \n",
    "    newGroundTruth.layer_input.weight=torch.nn.Parameter(layer_weight)\n",
    "    \n",
    "    \n",
    "    for i in range(GROUND_TRUTH_LAYERS):\n",
    "        layer_weight=torch.tensor([[0]*NEURONS]*NEURONS, requires_grad=False)\n",
    "        for bettermodel in better_model_list:\n",
    "            layer_weight=layer_weight+bettermodel.layers[i].weight/(len(better_model_list))\n",
    "        layer_weight.requires_grad_()\n",
    "        newGroundTruth.layers[i].weight=torch.nn.Parameter(layer_weight)\n",
    "        \n",
    "    layer_weight=torch.tensor([0]*NEURONS, requires_grad=False)\n",
    "    for i in better_model_list:\n",
    "        layer_weight=layer_weight+i.layer_output.weight/(len(better_model_list))\n",
    "    layer_weight.requires_grad_()   \n",
    "    newGroundTruth.layer_output.weight=torch.nn.Parameter(layer_weight)\n",
    "\n",
    "    return newGroundTruth\n",
    "        \n",
    "#checks accuracy of model\n",
    "def check_threshhold(network, test_loader, truth):\n",
    "    total_loss=0\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, y) in zip(test_loader, truth):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = network(X_batch)\n",
    "            y_pred_tag=y_test_pred\n",
    "            total_loss=total_loss+((y_pred_tag.cpu().numpy()-y))**2\n",
    "    \n",
    "    total_loss=total_loss/len(truth)\n",
    "\n",
    "    if total_loss[0][0]<THRESH_HOLD:\n",
    "        return True, total_loss[0][0]\n",
    "    else:\n",
    "        return False, total_loss[0][0]\n",
    "                       \n",
    "    \n",
    "       \n",
    "class arbitraryLayerModel(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super(arbitraryLayerModel, self).__init__()\n",
    "        self.layer_input=nn.Linear(INPUTS, NEURONS)\n",
    "        self.layers=nn.ModuleList([nn.Linear(NEURONS, NEURONS) for i in range(num_layers)])\n",
    "        \n",
    "        \n",
    "        self.batchnorms=nn.ModuleList([nn.BatchNorm1d(NEURONS) for i in range(num_layers)])\n",
    "        self.batchnorm_input=nn.BatchNorm1d(NEURONS)\n",
    "        \n",
    "        self.layer_output=nn.Linear(NEURONS,OUTPUT)\n",
    "    def get_copy(self):\n",
    "        copied_model=arbitraryLayerModel(len(self.layers))\n",
    "        copied_model.layers=nn.ModuleList()\n",
    "        for i in self.layers:\n",
    "            copied_model.layers.append(copy.deepcopy(i))\n",
    "            \n",
    "        copied_model.batchnorms=nn.ModuleList()\n",
    "        for i in self.batchnorms:\n",
    "            copied_model.batchnorms.append(copy.deepcopy(i))\n",
    "        copied_model.layer_input=copy.deepcopy(self.layer_input)\n",
    "        copied_model.layer_output=copy.deepcopy(self.layer_output)\n",
    "        copied_model.batchnorm_input=copy.deepcopy(self.batchnorm_input)\n",
    "\n",
    "        return copied_model\n",
    "            \n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        #x=self.activationlayer(self.batchnorm_input(self.layer_input(inputs)))\n",
    "        x=activation(self.layer_input(inputs))\n",
    "        for (layer, batchnorm) in zip(self.layers, self.batchnorms):\n",
    "            #x=self.activationlayer(batchnorm(layer(x)))\n",
    "            x=activation(layer(x))   \n",
    "                \n",
    "        x=self.layer_output(x)\n",
    "        \n",
    "        return(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    def change_node_model(self):\n",
    "        changed_node=self.get_copy()\n",
    "        NodeNum=randrange(NEURONS*(GROUND_TRUTH_LAYERS))\n",
    "        for i in range(NodeNum):\n",
    "            layeri=randrange(GROUND_TRUTH_LAYERS+2)\n",
    "            if layeri==0:\n",
    "                nodei=[randrange(INPUTS), randrange(NEURONS)]\n",
    "                layeri=1\n",
    "            if layeri==GROUND_TRUTH_LAYERS+1:\n",
    "                 nodei=[randrange(NEURONS), randrange(OUTPUT)]\n",
    "            else:\n",
    "                nodei=[randrange(NEURONS), randrange(NEURONS)]\n",
    "            node=[layeri, nodei]\n",
    "\n",
    "            weightsign=randrange(2)\n",
    "            if weightsign==0:\n",
    "                weightsign=-1\n",
    "                \n",
    "            weightchange=np.random.normal(weightsign*2,1)      \n",
    "\n",
    "            if node[0]==0:\n",
    "                zeros=[[0]*INPUTS]*NEURONS\n",
    "                zeros[node[1][1]][node[1][0]]=weightchange\n",
    "                newweight=self.layer_input.weight.requires_grad_(False)+torch.tensor(zeros,requires_grad=False)\n",
    "                newweight.requires_grad_()\n",
    "                changed_node.layer_input.weight=torch.nn.Parameter(newweight)    \n",
    "            elif node[0]==GROUND_TRUTH_LAYERS+1:\n",
    "                zeros=[[0]*(NEURONS)]*OUTPUT\n",
    "                zeros[node[1][1]][node[1][0]]=weightchange\n",
    "                newweight=self.layer_output.weight.requires_grad_(False)+torch.tensor(zeros,requires_grad=False)\n",
    "                newweight.requires_grad_()\n",
    "                changed_node.layer_output.weight=torch.nn.Parameter(newweight) \n",
    "\n",
    "            else:\n",
    "                zeros = [[0.0]*NEURONS]*NEURONS\n",
    "                for layer, i in enumerate(changed_node.layers):\n",
    "                    if i==node[0]+1:\n",
    "                        zeros[node[1][0]][node[1][1]]=weightchange\n",
    "                        newweight=layer.weight.requires_grad_(False)+torch.tensor(weightchange,requires_grad=False)\n",
    "                        newweight.requires_grad_()\n",
    "                        layer.weight=torch.nn.Parameter(newweight)\n",
    "\n",
    "\n",
    "\n",
    "        return changed_node\n",
    "\n",
    "def sine(data):\n",
    "    y=[]\n",
    "    for inputs in data:\n",
    "        answers=[]\n",
    "        for i in range(1, OUTPUT+1):\n",
    "            answers.append(np.sin(sum(inputs)*i))\n",
    "            \n",
    "        y.append(answers)\n",
    "    return y\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#only 1 ground truth generated\n",
    "Iteration_time_list=[]   \n",
    "current_time_list=[]\n",
    "data_list=[]\n",
    "\n",
    "GroundTruth = arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "\n",
    "#pretrain first ground truth\n",
    "if pretrained==True:\n",
    "    ground_truth_optim = optim.Adam(GroundTruth.parameters(), lr=LEARNING_RATE)\n",
    "    X=np.random.uniform( size=(10000, INPUTS))*2*np.pi\n",
    "    y=np.sin(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    GroundTruth.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    GroundTruth.train()\n",
    "    loss=100\n",
    "    while(loss>.05):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            ground_truth_optim.zero_grad()\n",
    "            y_pred = GroundTruth(X_batch)\n",
    "            #print(y_pred)\n",
    "            #print(y_batch)\n",
    "            loss = criterion(y_pred, target=y_batch)\n",
    "            loss.backward()\n",
    "            ground_truth_optim.step()\n",
    "            epoch_loss += loss.item()        \n",
    "\n",
    "        print(\"epoch loss for pretraining: \", epoch_loss/len(train_loader))\n",
    "        loss=epoch_loss/len(train_loader)\n",
    "\n",
    "\n",
    "possible_infinite_time=arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "firstmodeltrained=arbitraryLayerModel(TESTMODULE_LAYERS)\n",
    "test_model_optimizer = optim.SGD(firstmodeltrained.parameters(), lr=LEARNING_RATE)\n",
    "accepted=False\n",
    "\n",
    "\n",
    "\n",
    "while((not accepted)):\n",
    "    X, y=getdata_XY()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "    data_list.append(X)\n",
    "    trainnetwork(firstmodeltrained, train_loader,test_model_optimizer)\n",
    "    accepted, dummy=check_threshhold(firstmodeltrained, test_loader, y_test)\n",
    "    #print(dummy)\n",
    "\n",
    "\n",
    "\n",
    "current_time=len(data_list)\n",
    "print(\"First time: \", current_time)\n",
    "current_time_list.append(current_time)\n",
    "Iteration_time_list.append(current_time)\n",
    "\n",
    "#iterates\n",
    "interationcounter=1\n",
    "no_increasecounter=0\n",
    "time_increase=True\n",
    "loss_difference_counter=0\n",
    "stop_itterations=False\n",
    "\n",
    "while(time_increase):\n",
    "    time_list=[]\n",
    "    model_list=[]\n",
    "    test_model_list=[arbitraryLayerModel(TESTMODULE_LAYERS) for i in range(TESTMODELSGENERATED)]\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(TRUTHMODELSGENERATED):\n",
    "        data_test_list=[]\n",
    "        infinite_time_counter=0\n",
    "        prospective_GroundTruth=GroundTruth.change_node_model()\n",
    "        model_list.append(prospective_GroundTruth)\n",
    "        \n",
    "        for j, test_model in enumerate(test_model_list):\n",
    "            current_loss=0\n",
    "            previous_loss=0\n",
    "            data_counter=0\n",
    "            loss_difference_counter=0\n",
    "\n",
    "            \n",
    "            train_testmodel=test_model.get_copy()\n",
    "            test_model_optimizer = optim.SGD(train_testmodel.parameters(), lr=LEARNING_RATE)\n",
    "            inital_data_length=len(data_list)\n",
    "            \n",
    "            accepted=False\n",
    "            Model_beaten=False\n",
    "\n",
    "            while(not accepted): \n",
    "                #use previously generated data\n",
    "                if data_counter<inital_data_length:\n",
    "                    X=data_list[data_counter]\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "                    trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    previous_loss=current_loss\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=abs(previous_loss-current_loss)\n",
    "                    data_counter=data_counter+1\n",
    "                    test_time=data_counter\n",
    "                #generate new data if needed\n",
    "                else:                          \n",
    "                    X=get_Xvals()\n",
    "                    data_list.append(X)\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "                    #if len(data_list)%20==0:\n",
    "                    #    print(max(y)-min(y))\n",
    "\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "                    previous_loss=current_loss\n",
    "                    trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=abs(previous_loss-current_loss)\n",
    "                    test_time=len(data_list)\n",
    "\n",
    "\n",
    "                #checks if loss difference is small\n",
    "                if abs(loss_diff)<(THRESH_HOLD/BAD_LOSS_PASSES):\n",
    "                    loss_difference_counter=loss_difference_counter+1\n",
    "                    if loss_difference_counter>BAD_LOSS_PASSES:\n",
    "                        infinite_time_counter=infinite_time_counter+1\n",
    "                        print(\"Test model beaten. Total beaten by prospective ground truth: \", infinite_time_counter)\n",
    "                        break\n",
    "\n",
    "            time_list.append(test_time)\n",
    "\n",
    "    \n",
    "    max_new_time=sum(time_list)/TESTMODELSGENERATED\n",
    "    Iteration_time_list.append(max_new_time)      \n",
    "\n",
    "    #L2 distance from$\"0\"$\n",
    "    distance=0\n",
    "    distance=distance+prospective_GroundTruth.layer_input.weight.mul(prospective_GroundTruth.layer_input.weight).sum().item()\n",
    "    for layer in prospective_GroundTruth.layers:\n",
    "        distance=distance+layer.weight.mul(layer.weight).sum().item()\n",
    "    distance=distance+prospective_GroundTruth.layer_output.weight.mul(prospective_GroundTruth.layer_output.weight).sum().item()\n",
    "    distance=distance/(NEURONS*GROUND_TRUTH_LAYERS+INPUTS*NEURONS+OUTPUT*NEURONS)\n",
    "    \n",
    "    #singular values\n",
    "    product=torch.eye(NEURONS)\n",
    "    for layer in prospective_GroundTruth.layers:\n",
    "        product=product@layer.weight\n",
    "    numpproduct=product.detach().numpy()\n",
    "    u, s, vh = np.linalg.svd(numpproduct, full_matrices=True)\n",
    "    singular=s\n",
    "    \n",
    "    \n",
    "    #prints iteration data\n",
    "    print(\"Iteration\", interationcounter,\": | Data sizes: \", time_list)   \n",
    "    print(f'Iteration {interationcounter}: | Distance: {distance: .3f}')\n",
    "    print(f'Iteration {interationcounter}: | Previous time: {current_time:.3f} | Best new time: {max_new_time:.3f} | Difference: {max_new_time-current_time:.3f} | # beaten: {infinite_time_counter}')\n",
    "    interationcounter=interationcounter+1\n",
    "    \n",
    "    if infinite_time_counter==10:\n",
    "        print(\"Potential unbreakable found\")\n",
    "        possible_infinite_time=prospective_GroundTruth.get_copy()\n",
    "        print(\"Time: \", time_list)\n",
    "        break\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #changes ground truth if better time found\n",
    "    if max_new_time>current_time:\n",
    "        GroundTruth=model_list[0].get_copy()\n",
    "        time_increase=True\n",
    "        previous_time=current_time\n",
    "        current_time=max_new_time\n",
    "        no_increasecounter=0\n",
    "    else:\n",
    "        no_increasecounter=no_increasecounter+1\n",
    "\n",
    "    current_time_list.append(current_time)\n",
    "        \n",
    "    #if no better ground truth found for specified number, stops code                 \n",
    "    if no_increasecounter==NO_TIME_INCREASES_STOP:\n",
    "        print(\"No increase for \", NO_TIME_INCREASES_STOP,  \" iterations\")\n",
    "        time_increase=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cadba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
