{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d05ff88",
   "metadata": {
    "code_folding": [
     65,
     108,
     120,
     168,
     230,
     259,
     305
    ]
   },
   "outputs": [],
   "source": [
    "#import statements, hyper params, models\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from numpy import random\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import math\n",
    "import time\n",
    "\n",
    "THRESH_HOLD=.002          #MSE value when we consider the second model to have \"cracked\" the first\n",
    "EPOCHS = 50                #number of passes of whole data\n",
    "BATCH_SIZE =  50            #size of data going through at once\n",
    "LEARNING_RATE = 0.001        #how much we shift parameters during back prop\n",
    "NEURONS=1024                   #number of nodes in each layer\n",
    "INPUTS=1                 #number of input vars\n",
    "OUTPUT=1                    #number of output\n",
    "XSIZE=  100                 #size of each data set generated\n",
    "BAD_LOSS_PASSES=1000000        #number of times we have small loss until we stop code\n",
    "NO_TIME_INCREASES_STOP=100   #number of interations with no time increase until we stop code\n",
    "TESTMODULE_LAYERS=5        #number of layers in cracking module\n",
    "GROUND_TRUTH_LAYERS=5      #number of layers in ground thruth models\n",
    "LINCOMBO=False               #wether to take best new time or linear combination\n",
    "activation=nn.ReLU()    #activation  \n",
    "TRUTHMODELSGENERATED=1     #number of perturbed models\n",
    "TESTMODELSGENERATED=1\n",
    "pretrained=True\n",
    "DATATYPE=0                #0=data in [-1 ,1], rescaled; 1=sparse data; 2=periodic\n",
    "def get_Xvals():\n",
    "    if DATATYPE==0:\n",
    "        return np.random.uniform( size=(XSIZE, INPUTS))*2*np.pi\n",
    "    elif DATATYPE==1:\n",
    "        sparsedata=(np.random.uniform( size=(XSIZE, INPUTS)))*2*np.pi\n",
    "        for i in sparsedata:\n",
    "            for j in i:\n",
    "                if np.random.randint(0,2)==0:\n",
    "                    j=0\n",
    "        for i in sparsedata:\n",
    "            for j in i:\n",
    "                if np.random.randint(0,2)==0:\n",
    "                    j=0\n",
    "        return sparsedata\n",
    "    else:\n",
    "        return np.random.normal(size=(XSIZE, INPUTS))\n",
    "        \n",
    "\n",
    "                \n",
    "    \n",
    "def getdata_XY():\n",
    "    X=get_Xvals()\n",
    "    if DATATYPE==2:\n",
    "        dummy=[(i%1)*2*np.pi for i in X]\n",
    "    else:\n",
    "        dummy=X\n",
    "    test_data = testData(torch.FloatTensor(dummy))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    y_truthlist =[]\n",
    "    \n",
    "    \n",
    "    GroundTruth.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = GroundTruth(X_batch)\n",
    "            y_truthlist.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_truthlist = [a.squeeze().tolist() for a in y_truthlist]\n",
    "    return X, y_truthlist\n",
    "\n",
    "def get_yvals(model, X_data):\n",
    "    if DATATYPE==2:\n",
    "        X_data=[(i%1)*2*np.pi for i in X]\n",
    "    model.eval()\n",
    "    test_data = testData(torch.FloatTensor(X_data))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    y_truthlist=[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_truthlist.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_truthlist = [a.squeeze().tolist() for a in y_truthlist]\n",
    "    #print(\"MAX-MIN of yvals:\", max(y_truthlist)-min(y_truthlist))\n",
    "    #print(max(y_truthlist))\n",
    "    #print(min(y_truthlist))\n",
    "    return y_truthlist\n",
    "\n",
    "#Data class\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "#Data loaders (to look at data) and deal with data (training and testing)\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "#trains a network, returns time spent training \n",
    "def trainnetwork(network, train_loader, optimizer, Time=True):\n",
    "    network.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    network.train()\n",
    "    current_epoch=1\n",
    "    last_epoch=0\n",
    "    epoch_diff_counter=0\n",
    "    total_counter=0\n",
    "    while (epoch_diff_counter<EPOCHS and total_counter<EPOCHS*10) and current_epoch>THRESH_HOLD:\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = network(X_batch)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()        \n",
    "        \n",
    "        last_epoch=current_epoch\n",
    "        current_epoch=epoch_loss/len(train_loader)\n",
    "        if abs(last_epoch-current_epoch)<THRESH_HOLD/10:\n",
    "            epoch_diff_counter=epoch_diff_counter+1\n",
    "        else:\n",
    "            epoch_diff_counter=0\n",
    "        total_counter=total_counter+1\n",
    "            \n",
    "        #print(\"epoch loss: \", epoch_loss/len(train_loader))\n",
    "            \n",
    "\n",
    "#returns next ground truth model\n",
    "#model obtained by taking average of models with better times than previous model \n",
    "def get_new_GroundTruth_lincombo(model_list, time_list, time):\n",
    "    better_model_list=[]\n",
    "    for (i, model) in zip(time_list, model_list):\n",
    "        if i>time:\n",
    "            better_model_list.append(model)\n",
    "    \n",
    "    newGroundTruth=GroundTruth.get_copy()\n",
    "    \n",
    "    layer_weight=torch.tensor([[0]*INPUTS]*NEURONS, requires_grad=False)\n",
    "    for i in better_model_list:\n",
    "        layer_weight=layer_weight+i.layer_input.weight/(len(better_model_list))\n",
    "        \n",
    "    layer_weight.requires_grad_()    \n",
    "    newGroundTruth.layer_input.weight=torch.nn.Parameter(layer_weight)\n",
    "    \n",
    "    \n",
    "    for i in range(GROUND_TRUTH_LAYERS):\n",
    "        layer_weight=torch.tensor([[0]*NEURONS]*NEURONS, requires_grad=False)\n",
    "        for bettermodel in better_model_list:\n",
    "            layer_weight=layer_weight+bettermodel.layers[i].weight/(len(better_model_list))\n",
    "        layer_weight.requires_grad_()\n",
    "        newGroundTruth.layers[i].weight=torch.nn.Parameter(layer_weight)\n",
    "        \n",
    "    layer_weight=torch.tensor([0]*NEURONS, requires_grad=False)\n",
    "    for i in better_model_list:\n",
    "        layer_weight=layer_weight+i.layer_output.weight/(len(better_model_list))\n",
    "    layer_weight.requires_grad_()   \n",
    "    newGroundTruth.layer_output.weight=torch.nn.Parameter(layer_weight)\n",
    "\n",
    "    return newGroundTruth\n",
    "        \n",
    "#checks accuracy of model\n",
    "def check_threshhold(network, test_loader, truth):\n",
    "    total_loss=0\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, y) in zip(test_loader, truth):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = network(X_batch)\n",
    "            y_pred_tag=y_test_pred\n",
    "            total_loss=total_loss+((y_pred_tag.cpu().numpy()-y))**2\n",
    "    \n",
    "    total_loss=total_loss/len(truth)\n",
    "\n",
    "    if total_loss[0][0]<THRESH_HOLD:\n",
    "        return True, total_loss[0][0]\n",
    "    else:\n",
    "        return False, total_loss[0][0]\n",
    "                       \n",
    "    \n",
    "       \n",
    "class arbitraryLayerModel(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        super(arbitraryLayerModel, self).__init__()\n",
    "        self.layer_input=nn.Linear(INPUTS, NEURONS)\n",
    "        self.layers=nn.ModuleList([nn.Linear(NEURONS, NEURONS) for i in range(num_layers)])\n",
    "        \n",
    "        \n",
    "        self.batchnorms=nn.ModuleList([nn.BatchNorm1d(NEURONS) for i in range(num_layers)])\n",
    "        self.batchnorm_input=nn.BatchNorm1d(NEURONS)\n",
    "        \n",
    "        self.layer_output=nn.Linear(NEURONS,OUTPUT)\n",
    "    def get_copy(self):\n",
    "        copied_model=arbitraryLayerModel(len(self.layers))\n",
    "        copied_model.layers=nn.ModuleList()\n",
    "        for i in self.layers:\n",
    "            copied_model.layers.append(copy.deepcopy(i))\n",
    "            \n",
    "        copied_model.batchnorms=nn.ModuleList()\n",
    "        for i in self.batchnorms:\n",
    "            copied_model.batchnorms.append(copy.deepcopy(i))\n",
    "        copied_model.layer_input=copy.deepcopy(self.layer_input)\n",
    "        copied_model.layer_output=copy.deepcopy(self.layer_output)\n",
    "        copied_model.batchnorm_input=copy.deepcopy(self.batchnorm_input)\n",
    "\n",
    "        return copied_model\n",
    "            \n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        #x=self.activationlayer(self.batchnorm_input(self.layer_input(inputs)))\n",
    "        x=activation(self.layer_input(inputs))\n",
    "        for (layer, batchnorm) in zip(self.layers, self.batchnorms):\n",
    "            #x=self.activationlayer(batchnorm(layer(x)))\n",
    "            x=activation(layer(x))   \n",
    "                \n",
    "        x=self.layer_output(x)\n",
    "        \n",
    "        return(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    def change_node_model(self):\n",
    "        changed_node=self.get_copy()\n",
    "        NodeNum=randrange(NEURONS*(GROUND_TRUTH_LAYERS))\n",
    "        for i in range(NodeNum):\n",
    "            layeri=randrange(GROUND_TRUTH_LAYERS+2)\n",
    "            if layeri==0:\n",
    "                nodei=[randrange(INPUTS), randrange(NEURONS)]\n",
    "                layeri=1\n",
    "            if layeri==GROUND_TRUTH_LAYERS+1:\n",
    "                 nodei=[randrange(NEURONS), randrange(OUTPUT)]\n",
    "            else:\n",
    "                nodei=[randrange(NEURONS), randrange(NEURONS)]\n",
    "            node=[layeri, nodei]\n",
    "\n",
    "            weightsign=randrange(2)\n",
    "            if weightsign==0:\n",
    "                weightsign=-1\n",
    "                \n",
    "            weightchange=np.random.normal(weightsign*2,1)      \n",
    "\n",
    "            if node[0]==0:\n",
    "                zeros=[[0]*INPUTS]*NEURONS\n",
    "                zeros[node[1][1]][node[1][0]]=weightchange\n",
    "                newweight=self.layer_input.weight.requires_grad_(False)+torch.tensor(zeros,requires_grad=False)\n",
    "                newweight.requires_grad_()\n",
    "                changed_node.layer_input.weight=torch.nn.Parameter(newweight)    \n",
    "            elif node[0]==GROUND_TRUTH_LAYERS+1:\n",
    "                zeros=[[0]*(NEURONS)]*OUTPUT\n",
    "                zeros[node[1][1]][node[1][0]]=weightchange\n",
    "                newweight=self.layer_output.weight.requires_grad_(False)+torch.tensor(zeros,requires_grad=False)\n",
    "                newweight.requires_grad_()\n",
    "                changed_node.layer_output.weight=torch.nn.Parameter(newweight) \n",
    "\n",
    "            else:\n",
    "                zeros = [[0.0]*NEURONS]*NEURONS\n",
    "                for layer, i in enumerate(changed_node.layers):\n",
    "                    if i==node[0]+1:\n",
    "                        zeros[node[1][0]][node[1][1]]=weightchange\n",
    "                        newweight=layer.weight.requires_grad_(False)+torch.tensor(weightchange,requires_grad=False)\n",
    "                        newweight.requires_grad_()\n",
    "                        layer.weight=torch.nn.Parameter(newweight)\n",
    "\n",
    "\n",
    "\n",
    "        return changed_node\n",
    "\n",
    "def sine(data):\n",
    "    y=[]\n",
    "    for inputs in data:\n",
    "        answers=[]\n",
    "        for i in range(1, OUTPUT+1):\n",
    "            answers.append(np.sin(sum(inputs)*i))\n",
    "            \n",
    "        y.append(answers)\n",
    "    return y\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983e056",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#linear combo 1 weight guassian weight\n",
    "       \n",
    "#groundtruth model  \n",
    "GroundTruth = arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "possible_infinite_time=arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "\n",
    "\n",
    "\n",
    "firstmodeltrained=arbitraryLayerModel(TESTMODULE_LAYERS)\n",
    "test_model_optimizer = optim.SGD(firstmodeltrained.parameters(), lr=LEARNING_RATE)\n",
    "accepted=False\n",
    "current_time=0\n",
    "data_list=[]\n",
    "\n",
    "\n",
    "while((not accepted)):\n",
    "    X, y=getdata_XY(SPARSE)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "    data_list.append(X)\n",
    "    current_time=current_time+trainnetwork(firstmodeltrained, train_loader,test_model_optimizer)\n",
    "    accepted=check_threshhold(firstmodeltrained, test_loader, y_test)\n",
    "    \n",
    "print(\"First time: \", current_time)\n",
    "    \n",
    "#iterates\n",
    "interationcounter=1\n",
    "#number of iterates in a row with no time increase\n",
    "no_increasecounter=0\n",
    "time_increase=True\n",
    "#number of training interations where loss doesnt change\n",
    "loss_difference_counter=0\n",
    "stop_itterations=False\n",
    "while(time_increase):\n",
    "    node_list=[]\n",
    "    time_list=[]\n",
    "    model_list=[]\n",
    "    for i in range(0, 10):\n",
    "        node_list.append(select_node())\n",
    "    \n",
    "    test_model_list=[arbitraryLayerModel(TESTMODULE_LAYERS) for i in range(10)]\n",
    "    for i in range(0, 10):\n",
    "        total_time=0\n",
    "        infinite_time_counter=0\n",
    "        prospective_GroundTruth=GroundTruth.change_node_model(node_list[i])\n",
    "\n",
    "        model_list.append(prospective_GroundTruth)\n",
    "\n",
    "        for j, test_model in enumerate(test_model_list):\n",
    "            current_loss=0\n",
    "            previous_loss=0\n",
    "            train_testmodel=test_model.get_copy()\n",
    "            test_model_optimizer = optim.SGD(train_testmodel.parameters(), lr=LEARNING_RATE)\n",
    "            data_counter=0\n",
    "            test_time=0\n",
    "            inital_data_length=len(data_list)\n",
    "            accepted=False\n",
    "            loss_difference_counter=0\n",
    "            Model_beaten=False\n",
    "\n",
    "            while(not accepted): \n",
    "                #use previously generated data\n",
    "                if data_counter<inital_data_length:\n",
    "                    X=data_list[data_counter]\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    test_time=test_time+trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    previous_loss=current_loss\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=previous_loss-current_loss\n",
    "                    data_counter=data_counter+1\n",
    "                #generate new data if needed\n",
    "                else:                          \n",
    "                    X=get_Xvals(SPARSE)\n",
    "                    data_list.append(X)\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "\n",
    "\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "                    previous_loss=current_loss\n",
    "                    test_time=test_time+trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=previous_loss-current_loss\n",
    "\n",
    "\n",
    "                #checks if loss difference is small\n",
    "                \n",
    "                if abs(loss_diff)<(THRESH_HOLD/10):\n",
    "                    loss_difference_counter=loss_difference_counter+1\n",
    "                    if loss_difference_counter>BAD_LOSS_PASSES:\n",
    "                        infinite_time_counter=infinite_time_counter+1\n",
    "                        #print(\"Test model beaten. Total beaten by prospective ground truth: \", infinite_time_counter)\n",
    "                        Model_beaten=True\n",
    "                    #elif loss_difference_counter>5:\n",
    "                        #print(f'Number no loss change {loss_difference_counter}: | Current time: {test_time:.3f} | Loss difference: {abs(loss_diff):.4} | Loss: {current_loss:.4}')\n",
    " \n",
    "            \n",
    "                if Model_beaten:\n",
    "                    break\n",
    "\n",
    "\n",
    "            total_time=total_time+test_time\n",
    "\n",
    "    \n",
    "    \n",
    "        print(f'Iteration: {interationcounter} | Ground truth number: {i+1} | Average time:  {total_time/10:.3}')\n",
    "\n",
    "        if infinite_time_counter==10:\n",
    "            print(\"Potential unbreakable found\")\n",
    "            possible_infinite_time=prospective_GroundTruth.get_copy()\n",
    "            break\n",
    "        time_list.append(total_time/(10))\n",
    "\n",
    "        \n",
    "           \n",
    "    \n",
    "    if infinite_time_counter==10:\n",
    "        break\n",
    "    max_new_time=max(time_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #prints iteration data\n",
    "    print(f'Iteration {interationcounter}: | Previous time: {current_time:.3f} | Best new time: {max_new_time:.3f} | Difference: {max_new_time-current_time:.3f}')\n",
    "    interationcounter=interationcounter+1\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    time_increase=max_new_time-current_time\n",
    "    \n",
    "    \n",
    "    \n",
    "    #changes ground truth if better time found\n",
    "    if max_new_time>current_time and max_new_time<current_time*50:\n",
    "        if LINCOMBO:\n",
    "            GroundTruth=get_new_GroundTruth_lincombo(model_list, time_list, current_time)\n",
    "        else:\n",
    "            maxindex=time_list.index(max(time_list))\n",
    "            GroundTruth=model_list[maxindex].get_copy()\n",
    "        time_increase=True\n",
    "        previous_time=current_time\n",
    "        current_time=max_new_time\n",
    "        no_increasecounter=0\n",
    "        \n",
    "    else:\n",
    "        no_increasecounter=no_increasecounter+1\n",
    "\n",
    "    #if no better ground truth found for 10 iterations, stops code                 \n",
    "    if no_increasecounter==NO_TIME_INCREASES_STOP:\n",
    "        print(\"No increase for \", NO_TIME_INCREASES_STOP,  \" iterations\")\n",
    "    #    time_increase=False\n",
    " \n",
    "      \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57aac7f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2724404\n",
      "0.18741068\n",
      "0.1331542\n",
      "0.18148789\n",
      "0.18351008\n",
      "0.19293392\n",
      "0.13349484\n",
      "0.117776744\n",
      "0.14320733\n",
      "0.105365336\n",
      "0.104261495\n",
      "0.16165653\n",
      "0.08392844\n",
      "0.08962713\n",
      "0.119149595\n",
      "0.11808546\n",
      "0.14264849\n",
      "0.13018303\n",
      "0.14442861\n",
      "0.12791997\n",
      "0.0952644\n",
      "0.11159464\n",
      "0.106953315\n",
      "0.16257614\n",
      "0.09594859\n",
      "0.1324545\n",
      "0.11269296\n",
      "0.103851125\n",
      "0.100530684\n",
      "0.098445565\n",
      "0.11002023\n",
      "0.13552432\n",
      "0.10788508\n",
      "0.12894097\n",
      "0.09049782\n",
      "0.10331824\n",
      "0.10322793\n",
      "0.08121519\n",
      "0.07845735\n",
      "0.095827214\n",
      "0.090935014\n",
      "0.08519224\n",
      "0.09607724\n",
      "0.07144291\n",
      "0.095856875\n",
      "0.098651096\n",
      "0.087323025\n",
      "0.085469924\n",
      "0.081346676\n",
      "0.08094725\n",
      "0.10080024\n",
      "0.11472139\n",
      "0.13188587\n",
      "0.075629644\n",
      "0.13582623\n",
      "0.09179469\n",
      "0.08780888\n",
      "0.105124205\n",
      "0.12550363\n",
      "0.07537703\n",
      "0.041627157\n",
      "0.074178696\n",
      "0.06394231\n",
      "0.087706864\n",
      "0.086343765\n",
      "0.08324981\n",
      "0.08439309\n",
      "0.053635992\n",
      "0.08247913\n",
      "0.08828798\n",
      "0.08912863\n",
      "0.075995125\n",
      "0.06905014\n",
      "0.14365026\n",
      "0.071211696\n",
      "0.055223383\n",
      "0.10850399\n",
      "0.06728316\n",
      "0.09505701\n",
      "0.09836635\n",
      "0.04528232\n",
      "0.07255344\n",
      "0.073038645\n",
      "0.066527024\n",
      "0.063559234\n",
      "0.06445867\n",
      "0.09866294\n",
      "0.06747971\n",
      "0.051867362\n",
      "0.04525609\n",
      "0.04207414\n",
      "0.023182934\n",
      "0.0765342\n",
      "0.05696906\n",
      "0.059768375\n",
      "0.08880201\n",
      "0.04441818\n",
      "0.05116004\n",
      "0.04766576\n",
      "0.031103015\n",
      "0.055864997\n",
      "0.04142458\n",
      "0.056272026\n",
      "0.073085725\n",
      "0.061621804\n",
      "0.032773994\n",
      "0.07431634\n",
      "0.039084706\n",
      "0.047851097\n",
      "0.059427146\n",
      "0.020541348\n",
      "0.0632829\n",
      "0.04201203\n",
      "0.04595375\n",
      "0.0495129\n",
      "0.03419655\n",
      "0.059212975\n",
      "0.04260025\n",
      "0.042591073\n",
      "0.031463135\n",
      "0.07236396\n",
      "0.048861057\n",
      "0.023961473\n",
      "0.04815159\n",
      "0.04184296\n",
      "0.028996035\n",
      "0.029131282\n",
      "0.0404498\n",
      "0.08200338\n",
      "0.0458763\n",
      "0.052300032\n",
      "0.04161748\n",
      "0.019194521\n",
      "0.05009582\n",
      "0.03435238\n",
      "0.05777294\n",
      "0.044749375\n",
      "0.032843746\n",
      "0.01594838\n",
      "0.03961727\n",
      "0.027845697\n",
      "0.03688814\n",
      "0.0397969\n",
      "0.012750524\n",
      "0.047035504\n",
      "0.023629094\n",
      "0.02388147\n",
      "0.028652525\n",
      "0.04482406\n",
      "0.029393144\n",
      "0.03169657\n",
      "0.028562628\n",
      "0.025010483\n",
      "0.020815942\n",
      "0.033531297\n",
      "0.04179861\n",
      "0.018163828\n",
      "0.028057385\n",
      "0.021066716\n",
      "0.016826043\n",
      "0.018397039\n",
      "0.024207905\n",
      "0.024461884\n",
      "0.031862766\n",
      "0.02749915\n",
      "0.019209808\n",
      "0.010493116\n",
      "0.01860378\n",
      "0.025350356\n",
      "0.017330874\n",
      "0.017107408\n",
      "0.028409302\n",
      "0.023864483\n",
      "0.0063703526\n",
      "0.024148662\n",
      "0.012195092\n",
      "0.015912775\n",
      "0.014945021\n",
      "0.015548151\n",
      "0.010961802\n",
      "0.013363\n",
      "0.018519586\n",
      "0.00964666\n",
      "0.007571495\n",
      "0.006127972\n",
      "0.009106204\n",
      "0.014847604\n",
      "0.019305909\n",
      "0.016233705\n",
      "0.010751149\n",
      "0.012395808\n",
      "0.008683729\n",
      "0.019885479\n",
      "0.0054357355\n",
      "0.010335727\n",
      "0.009065825\n",
      "0.007485069\n",
      "0.0044345804\n",
      "0.0059502423\n",
      "0.0071579856\n",
      "0.0057592536\n",
      "0.006458738\n",
      "0.009583948\n",
      "0.014777776\n",
      "0.008097586\n",
      "0.0047479547\n",
      "0.007204239\n",
      "0.0072368965\n",
      "0.0079924865\n",
      "0.009257377\n",
      "0.008017279\n",
      "0.008989714\n",
      "0.005562344\n",
      "0.007365651\n",
      "0.004243752\n",
      "0.0075365524\n",
      "0.0094883945\n",
      "0.006919628\n",
      "0.0032443493\n",
      "0.0075000036\n",
      "0.0070036966\n",
      "0.002967413\n",
      "0.0053466614\n",
      "0.007322364\n",
      "0.00622619\n",
      "0.004522354\n",
      "0.0060092746\n",
      "0.006400168\n",
      "0.005621596\n",
      "0.0039363303\n",
      "0.0058814804\n",
      "0.009190413\n",
      "0.003794662\n",
      "0.0015098331\n",
      "First time:  234\n",
      "Iteration 1 : | Data sizes:  [234]\n",
      "Iteration 1: | Distance:  0.300\n",
      "Iteration 1: | Previous time: 234.000 | Best new time: 234.000 | Difference: 0.000 | # beaten: 0\n",
      "1.7830896973609924\n",
      "Iteration 2 : | Data sizes:  [251]\n",
      "Iteration 2: | Distance:  0.301\n",
      "Iteration 2: | Previous time: 234.000 | Best new time: 251.000 | Difference: 17.000 | # beaten: 0\n",
      "Iteration 3 : | Data sizes:  [76]\n",
      "Iteration 3: | Distance:  0.302\n",
      "Iteration 3: | Previous time: 251.000 | Best new time: 76.000 | Difference: -175.000 | # beaten: 0\n",
      "1.1672935038805008\n",
      "1.164695680141449\n",
      "1.1673496067523956\n",
      "1.167231261730194\n",
      "1.1673084646463394\n",
      "1.167318657040596\n",
      "1.1673931926488876\n",
      "Iteration 4 : | Data sizes:  [397]\n",
      "Iteration 4: | Distance:  0.301\n",
      "Iteration 4: | Previous time: 251.000 | Best new time: 397.000 | Difference: 146.000 | # beaten: 0\n",
      "1.1670694649219513\n",
      "Iteration 5 : | Data sizes:  [417]\n",
      "Iteration 5: | Distance:  0.301\n",
      "Iteration 5: | Previous time: 397.000 | Best new time: 417.000 | Difference: 20.000 | # beaten: 0\n",
      "0.9783184248954058\n",
      "0.9662253651767969\n",
      "0.9591029100120068\n",
      "0.9786265194416046\n",
      "0.9712900277227163\n",
      "0.9783418402075768\n",
      "0.9638763274997473\n",
      "0.9784164968878031\n",
      "0.9706681333482265\n",
      "0.9655351210385561\n",
      "0.955201119184494\n",
      "0.9611097164452076\n",
      "0.9748014267534018\n",
      "0.9779238142073154\n",
      "0.9621341452002525\n",
      "Iteration 6 : | Data sizes:  [716]\n",
      "Iteration 6: | Distance:  0.302\n",
      "Iteration 6: | Previous time: 417.000 | Best new time: 716.000 | Difference: 299.000 | # beaten: 0\n",
      "Iteration 7 : | Data sizes:  [440]\n",
      "Iteration 7: | Distance:  0.302\n",
      "Iteration 7: | Previous time: 716.000 | Best new time: 440.000 | Difference: -276.000 | # beaten: 0\n",
      "0.9787277709692717\n",
      "0.978516424074769\n",
      "Iteration 8 : | Data sizes:  [746]\n",
      "Iteration 8: | Distance:  0.303\n",
      "Iteration 8: | Previous time: 716.000 | Best new time: 746.000 | Difference: 30.000 | # beaten: 0\n"
     ]
    }
   ],
   "source": [
    "#only 1 ground truth generated\n",
    "Iteration_time_list=[]   \n",
    "current_time_list=[]\n",
    "data_list=[]\n",
    "\n",
    "GroundTruth = arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "\n",
    "#pretrain first ground truth\n",
    "if pretrained==True:\n",
    "    ground_truth_optim = optim.Adam(GroundTruth.parameters(), lr=LEARNING_RATE)\n",
    "    X=np.random.uniform( size=(10000, INPUTS))*2*np.pi\n",
    "    y=np.sin(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    GroundTruth.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    GroundTruth.train()\n",
    "    loss=100\n",
    "    while(loss>.05):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            ground_truth_optim.zero_grad()\n",
    "            y_pred = GroundTruth(X_batch)\n",
    "            #print(y_pred)\n",
    "            #print(y_batch)\n",
    "            loss = criterion(y_pred, target=y_batch)\n",
    "            loss.backward()\n",
    "            ground_truth_optim.step()\n",
    "            epoch_loss += loss.item()        \n",
    "\n",
    "        print(\"epoch loss: \", epoch_loss/len(train_loader))\n",
    "        loss=epoch_loss/len(train_loader)\n",
    "\n",
    "\n",
    "possible_infinite_time=arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "firstmodeltrained=arbitraryLayerModel(TESTMODULE_LAYERS)\n",
    "test_model_optimizer = optim.SGD(firstmodeltrained.parameters(), lr=LEARNING_RATE)\n",
    "accepted=False\n",
    "\n",
    "\n",
    "\n",
    "while((not accepted)):\n",
    "    X, y=getdata_XY()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "    data_list.append(X)\n",
    "    trainnetwork(firstmodeltrained, train_loader,test_model_optimizer)\n",
    "    accepted, dummy=check_threshhold(firstmodeltrained, test_loader, y_test)\n",
    "    print(dummy)\n",
    "\n",
    "\n",
    "\n",
    "current_time=len(data_list)\n",
    "print(\"First time: \", current_time)\n",
    "current_time_list.append(current_time)\n",
    "Iteration_time_list.append(current_time)\n",
    "\n",
    "#iterates\n",
    "interationcounter=1\n",
    "no_increasecounter=0\n",
    "time_increase=True\n",
    "loss_difference_counter=0\n",
    "stop_itterations=False\n",
    "\n",
    "while(time_increase):\n",
    "    time_list=[]\n",
    "    model_list=[]\n",
    "    test_model_list=[arbitraryLayerModel(TESTMODULE_LAYERS) for i in range(TESTMODELSGENERATED)]\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(TRUTHMODELSGENERATED):\n",
    "        data_test_list=[]\n",
    "        infinite_time_counter=0\n",
    "        prospective_GroundTruth=GroundTruth.change_node_model()\n",
    "        model_list.append(prospective_GroundTruth)\n",
    "        \n",
    "        for j, test_model in enumerate(test_model_list):\n",
    "            current_loss=0\n",
    "            previous_loss=0\n",
    "            data_counter=0\n",
    "            loss_difference_counter=0\n",
    "\n",
    "            \n",
    "            train_testmodel=test_model.get_copy()\n",
    "            test_model_optimizer = optim.SGD(train_testmodel.parameters(), lr=LEARNING_RATE)\n",
    "            inital_data_length=len(data_list)\n",
    "            \n",
    "            accepted=False\n",
    "            Model_beaten=False\n",
    "\n",
    "            while(not accepted): \n",
    "                #use previously generated data\n",
    "                if data_counter<inital_data_length:\n",
    "                    X=data_list[data_counter]\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "                    trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    previous_loss=current_loss\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=abs(previous_loss-current_loss)\n",
    "                    data_counter=data_counter+1\n",
    "                    test_time=data_counter\n",
    "                #generate new data if needed\n",
    "                else:                          \n",
    "                    X=get_Xvals()\n",
    "                    data_list.append(X)\n",
    "                    y=get_yvals(prospective_GroundTruth, X)\n",
    "                    #if len(data_list)%20==0:\n",
    "                    #    print(max(y)-min(y))\n",
    "\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "                    train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "                    test_data = testData(torch.FloatTensor(X_test))\n",
    "                    train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)                      \n",
    "                    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "                    previous_loss=current_loss\n",
    "                    trainnetwork(train_testmodel, train_loader,test_model_optimizer)\n",
    "                    accepted, current_loss=check_threshhold(train_testmodel, test_loader, y_test)\n",
    "                    loss_diff=abs(previous_loss-current_loss)\n",
    "                    test_time=len(data_list)\n",
    "\n",
    "\n",
    "                #checks if loss difference is small\n",
    "                if abs(loss_diff)<(THRESH_HOLD/BAD_LOSS_PASSES):\n",
    "                    loss_difference_counter=loss_difference_counter+1\n",
    "                    if loss_difference_counter>BAD_LOSS_PASSES:\n",
    "                        infinite_time_counter=infinite_time_counter+1\n",
    "                        print(\"Test model beaten. Total beaten by prospective ground truth: \", infinite_time_counter)\n",
    "                        break\n",
    "\n",
    "            time_list.append(test_time)\n",
    "\n",
    "    \n",
    "    max_new_time=sum(time_list)/TESTMODELSGENERATED\n",
    "    Iteration_time_list.append(max_new_time)      \n",
    "\n",
    "    #L2 distance from$\"0\"$\n",
    "    distance=0\n",
    "    distance=distance+prospective_GroundTruth.layer_input.weight.mul(prospective_GroundTruth.layer_input.weight).sum().item()\n",
    "    for layer in prospective_GroundTruth.layers:\n",
    "        distance=distance+layer.weight.mul(layer.weight).sum().item()\n",
    "    distance=distance+prospective_GroundTruth.layer_output.weight.mul(prospective_GroundTruth.layer_output.weight).sum().item()\n",
    "    distance=distance/(NEURONS*GROUND_TRUTH_LAYERS+INPUTS*NEURONS+OUTPUT*NEURONS)\n",
    "    \n",
    "    #singular values\n",
    "    product=torch.eye(NEURONS)\n",
    "    for layer in prospective_GroundTruth.layers:\n",
    "        product=product@layer.weight\n",
    "    numpproduct=product.detach().numpy()\n",
    "    u, s, vh = np.linalg.svd(numpproduct, full_matrices=True)\n",
    "    singular=s\n",
    "    \n",
    "    \n",
    "    #prints iteration data\n",
    "    print(\"Iteration\", interationcounter,\": | Data sizes: \", time_list)   \n",
    "    print(f'Iteration {interationcounter}: | Distance: {distance: .3f}')\n",
    "    print(f'Iteration {interationcounter}: | Previous time: {current_time:.3f} | Best new time: {max_new_time:.3f} | Difference: {max_new_time-current_time:.3f} | # beaten: {infinite_time_counter}')\n",
    "    interationcounter=interationcounter+1\n",
    "    \n",
    "    if infinite_time_counter==10:\n",
    "        print(\"Potential unbreakable found\")\n",
    "        possible_infinite_time=prospective_GroundTruth.get_copy()\n",
    "        print(\"Time: \", time_list)\n",
    "        break\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #changes ground truth if better time found\n",
    "    if max_new_time>current_time:\n",
    "        GroundTruth=model_list[0].get_copy()\n",
    "        time_increase=True\n",
    "        previous_time=current_time\n",
    "        current_time=max_new_time\n",
    "        no_increasecounter=0\n",
    "    else:\n",
    "        no_increasecounter=no_increasecounter+1\n",
    "\n",
    "    current_time_list.append(current_time)\n",
    "        \n",
    "    #if no better ground truth found for specified number, stops code                 \n",
    "    if no_increasecounter==NO_TIME_INCREASES_STOP:\n",
    "        print(\"No increase for \", NO_TIME_INCREASES_STOP,  \" iterations\")\n",
    "        time_increase=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a88d4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#sparse traing and nonsparse test\n",
    "GroundTruth\n",
    "firstmodeltrained\n",
    "sparsedata=(np.random.uniform( size=(10000, INPUTS))-0.5)*np.sqrt(12)\n",
    "for i in sparsedata:\n",
    "    for j in i:\n",
    "        if np.random.randint(0,2)==0:\n",
    "                j=0\n",
    "for i in sparsedata:\n",
    "    for j in i:\n",
    "        if np.random.randint(0,2)==0:\n",
    "            j=0\n",
    "X=sparsedata\n",
    "testmodel1=arbitraryLayerModel(TESTMODULE_LAYERS)\n",
    "testmodel2=testmodel1.get_copy()\n",
    "\n",
    "yvals1=get_yvals(GroundTruth, X)\n",
    "yvals2=get_yvals(firstmodeltrained, X)\n",
    "\n",
    "\n",
    "\n",
    "#\"complicated mode\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yvals1, test_size=0.2, shuffle=False)\n",
    "train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "test_model_optimizer = optim.SGD(testmodel1.parameters(), lr=LEARNING_RATE)\n",
    "trainnetwork(testmodel1, train_loader,test_model_optimizer)\n",
    "print(check_threshhold(testmodel1, test_loader, y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yvals2, test_size=0.2, shuffle=False)\n",
    "train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "test_model_optimizer = optim.SGD(testmodel2.parameters(), lr=LEARNING_RATE)\n",
    "trainnetwork(testmodel2, train_loader,test_model_optimizer)\n",
    "print(check_threshhold(testmodel2, test_loader, y_test))\n",
    "\n",
    "\n",
    "X=(np.random.uniform( size=(3000, INPUTS))-0.5)*np.sqrt(12)\n",
    "yvalstrue1=get_yvals(GroundTruth, X)\n",
    "yvalstrue2=get_yvals(firstmodeltrained, X)\n",
    "yvalstest1=get_yvals(testmodel1, X)\n",
    "yvalstest2=get_yvals(testmodel2, X)\n",
    "MSE1=mean_squared_error(yvalstrue1,yvalstest1)\n",
    "MSE2=mean_squared_error(yvalstrue2,yvalstest2)\n",
    "print(\"MSE complicated:\", MSE1)\n",
    "print(\"MSE simple:\", MSE2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0dc70",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#tests\n",
    "sparsedata=(np.random.uniform( size=(1000, INPUTS))-0.5)*np.sqrt(12)\n",
    "\n",
    "X=sparsedata\n",
    "testmodel1=arbitraryLayerModel(TESTMODULE_LAYERS)\n",
    "testmodel2=testmodel1.get_copy()\n",
    "\n",
    "yvals1=get_yvals(smart, X)\n",
    "yvals2=get_yvals(dumb, X)\n",
    "\n",
    "\n",
    "\n",
    "#\"complicated mode\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yvals1, test_size=0.2, shuffle=False)\n",
    "train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "\n",
    "test_model_optimizer = optim.SGD(testmodel1.parameters(), lr=LEARNING_RATE)\n",
    "trainnetwork(testmodel1, train_loader,test_model_optimizer)\n",
    "print(check_threshhold(testmodel1, test_loader, y_test))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yvals2, test_size=0.2, shuffle=False)\n",
    "train_data= trainData(torch.FloatTensor(X_train),torch.FloatTensor(y_train))\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "train_loader= DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)                      \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "\n",
    "test_model_optimizer = optim.SGD(testmodel2.parameters(), lr=LEARNING_RATE)\n",
    "trainnetwork(testmodel2, train_loader,test_model_optimizer)\n",
    "print(check_threshhold(testmodel2, test_loader, y_test))\n",
    "\n",
    "\n",
    "X=(np.random.uniform( size=(3000, INPUTS))-0.5)*np.sqrt(12)\n",
    "yvalstrue1=get_yvals(smart, X)\n",
    "yvalstrue2=get_yvals(dumb, X)\n",
    "yvalstest1=get_yvals(testmodel1, X)\n",
    "yvalstest2=get_yvals(testmodel2, X)\n",
    "MSE1=mean_squared_error(yvalstrue1,yvalstest1)\n",
    "MSE2=mean_squared_error(yvalstrue2,yvalstest2)\n",
    "print(\"MSE complicated:\", MSE1)\n",
    "print(\"MSE simple:\", MSE2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf22e7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#can scale weights to whatever u want\n",
    "GroundTruth = arbitraryLayerModel(GROUND_TRUTH_LAYERS)\n",
    "newinput=torch.rand_like(GroundTruth.layer_input.weight)\n",
    "GroundTruth.layer_input.weight=torch.nn.Parameter(newinput)\n",
    "\n",
    "for layer in GroundTruth.layers:\n",
    "    newweight=torch.rand_like(layer.weight)\n",
    "    layer.weight=torch.nn.Parameter(newweight)\n",
    "newoutput=torch.rand_like(GroundTruth.layer_output.weight)\n",
    "GroundTruth.layer_output.weight=torch.nn.Parameter(newoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e0f8fed",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss:  0.11100735738291405\n",
      "epoch loss:  0.013306359730540862\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1842",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#graphs\n",
    "X=np.linspace(0, 2*np.pi, num=1000)\n",
    "y=np.sin(X)*1/2+1/2\n",
    "plt.plot(X, y)\n",
    "plt.show()\n",
    "X_test=[[i] for i in X]\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "GroundTruth.eval()\n",
    "y_truthlist=[]\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = GroundTruth(X_batch)\n",
    "        y_truthlist.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "y_truthlist = [a.squeeze().tolist() for a in y_truthlist]\n",
    "plt.plot(X, y_truthlist)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976efad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GroundTruth.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer=optim.SGD(GroundTruth.parameters(), lr=LEARNING_RATE)\n",
    "GroundTruth.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        print(X_batch)\n",
    "        print(y_batch.unsqueeze(1))\n",
    "        y_pred = GroundTruth(X_batch)\n",
    "        loss = criterion(y_pred, target=y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()        \n",
    "\n",
    "    #print(\"epoch loss: \", epoch_loss/len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.uniform( size=(1000, INPUTS))*2*np.pi\n",
    "y=sine(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(2, 1, requires_grad=True)\n",
    "target = torch.randn(2, 1)\n",
    "print(input)\n",
    "print(target)\n",
    "print(criterion(input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c30891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
